# Chatbot RAG – System Architecture and Developer Guide

This document explains the folder structure, core components, execution flows (CLI and Web), configuration, and diagnostics of the Chatbot RAG project. Reading this should give you a clear understanding of how the system works end-to-end.

## 1) High-Level Overview

This project implements a Retrieval-Augmented Generation (RAG) chatbot:
- Documents in `knowledge_base/` are ingested into a FAISS vector index with SentenceTransformer embeddings.
- At query time, the system retrieves relevant chunks from FAISS and passes them to a local LLM (via Ollama) using a prompt template to generate the final answer.
- The app can run from the command line (interactive CLI) or via a web interface (FastAPI backend + React frontend).

Key properties:
- CPU-only compatible
- Local-only by design (privacy by default)
- Clear separation of concerns between ingestion, retrieval, LLM, and UI

## 2) Tech Stack
- Python 3.11+
- LangChain, LangChain Community integrations
- FAISS (vector similarity search)
- SentenceTransformers (embeddings: `all-MiniLM-L6-v2`)
- Ollama (LLM runner; model set via environment, e.g., `tinydolphin` or `mistral`)
- FastAPI (backend) + Uvicorn
- React (frontend)

## 3) Directory Layout

```
chatbot_RAG/
├── ARCHITECTURE.md            # This document
├── README.md                  # Quick-start and usage
├── requirements.txt           # Python dependencies
├── .gitignore                 # Global project ignores
├── knowledge_base/            # Your source documents (PDF/TXT)
├── faiss_index/               # Persisted FAISS index (generated by ingestion)
├── logs/                      # Application logs
├── data_loader.py             # Document loading + splitting
├── vector_store.py            # Embedding model + FAISS create/load
├── rag_pipeline.py            # LLM setup + RAG chain construction
├── main.py                    # CLI entrypoint (ingest/chat/health/web)
├── config/
│   ├── __init__.py
│   └── settings.py            # Pydantic settings (reads from environment)
├── utils/
│   ├── cache.py               # Lightweight in-memory cache helpers (optional use)
│   ├── logging_config.py      # Structured logging setup
│   ├── monitoring.py          # Metrics & health aggregation
│   ├── performance.py         # Perf helpers (optional)
│   └── warmup.py              # Warmup helpers (optional)
├── tests/
│   ├── __init__.py
│   ├── test_monitoring.py     # Unit tests for metrics/health
│   └── test_validation.py     # Unit tests for validation (if used)
└── web/
    ├── backend/
    │   └── main.py            # FastAPI app, WebSocket /chat endpoint
    └── frontend/
        ├── public/            # Static assets
        ├── src/               # React app sources
        └── .gitignore
```

## 4) Configuration
`config/settings.py` centralizes configuration using Pydantic settings and environment variables (`.env`). Typical fields include:
- Paths: `knowledge_base_dir`, `faiss_index_path`, `log_file`
- Embeddings: `embedding_model_name`, `embedding_device`
- Text splitting: `chunk_size`, `chunk_overlap`
- Retrieval: `search_k`
- LLM/Ollama: `ollama_model_name`, `ollama_base_url`, `ollama_timeout`

How to use in code:
```python
from config.settings import get_settings
settings = get_settings()
print(settings.ollama_model_name)
```

## 5) Ingestion Flow (Build the Vector Index)
Purpose: Convert documents in `knowledge_base/` into a searchable FAISS index.

- `data_loader.load_documents()`
  - Loads all `.pdf` and `.txt` files from `knowledge_base/` using LangChain loaders.
- `data_loader.split_documents()`
  - Splits documents into overlapping chunks (configurable size/overlap) to improve retrieval granularity.
- `vector_store.get_embedding_model()`
  - Creates the SentenceTransformer embedding model (CPU device by default).
- `FAISS.from_documents(...)`
  - Embeds the chunks and builds an in-memory FAISS index.
- `vector_store.save_local(...)`
  - Saves index to `faiss_index/` for later reuse.

Run from CLI:
```bash
python main.py ingest
```
On success: `faiss_index/` will contain the serialized index.

## 6) Query-Time Flow (RAG)
Purpose: Answer user questions using retrieved context + LLM.

- `vector_store.load_vector_store()`
  - Loads FAISS index and the embedding model.
- `rag_pipeline.get_ollama_llm()`
  - Creates the Ollama LLM client (model name from settings/env).
- `rag_pipeline.create_rag_chain(vector_store)`
  - Creates a chain: retrieve top-K chunks → format prompt → call LLM → return string.
  - Prompt template (concise): includes `Context` + `Question`.

From CLI:
```bash
python main.py chat
```
Loop:
- Reads input from console
- Validates/sanitizes (if validation enabled)
- Invokes the RAG chain and prints the answer

## 7) Web Server Flow (FastAPI + WebSocket)
Purpose: Provide a browser UI with streaming responses.

- Start server:
```bash
python main.py web
```
- Backend: `web/backend/main.py`
  - On startup (`@app.on_event("startup")`):
    - Load vector store and create the RAG chain once
    - Store the chain in `app.state.rag_chain`
  - WebSocket `/chat`:
    - Accept connection
    - Read text messages from the client
    - Stream chunks back using `rag_chain.astream(query)`
- Frontend: `web/frontend` (React)
  - Connects to `ws://<host>/chat`
  - Sends message on submit
  - Appends streamed chunks for a live typing effect

If you see a warning similar to:
```
WARNING: No supported WebSocket library detected.
```
Install WebSocket extras:
```bash
pip install "uvicorn[standard]" websockets
```

## 8) Logging & Monitoring
- `utils/logging_config.py` sets up structured logs for console + optional file rotation.
- `utils/monitoring.py` tracks metrics like:
  - Total/success/failed requests
  - Average response time
  - CPU/memory/disk usage
  - Recent error counts
- CLI health check:
```bash
python main.py health
```
Outputs a quick snapshot of system health.

## 9) Performance Guidance (CPU-only)
- Prefer a smaller Ollama model (e.g., `tinydolphin`) for faster responses on CPU.
- Reduce `search_k` if you see high latency from retrieval/formatting.
- Tune `chunk_size` and `chunk_overlap` to balance relevance vs. speed.
- Keep the backend process warm (avoid frequent restarts) so models stay in memory.

## 10) Common Workflows

### A) First-time Setup
1) Ensure Ollama is installed and running
2) Pull a model, e.g. `ollama pull tinydolphin`
3) `pip install -r requirements.txt`
4) (Optional) Copy `env.example` → `.env` and adjust settings
5) `python main.py ingest` to build the FAISS index

### B) CLI Chat
```bash
python main.py chat
```

### C) Web Chat
```bash
python main.py web
# Visit http://localhost:8000
```

## 11) Troubleshooting
- "Vector store not found": Run `python main.py ingest`
- "Ollama connection failed": Ensure the Ollama app is running; verify model name in `.env`
- WebSocket errors:
  - Install extras: `pip install "uvicorn[standard]" websockets`
  - Confirm browser is connecting to `ws://localhost:8000/chat`
- Slow responses on CPU:
  - Switch to a smaller LLM (e.g., `tinydolphin`)
  - Lower `search_k` and reduce chunk size

## 12) Testing
- Run unit tests:
```bash
pytest -v
```
- Coverage (if configured):
```bash
pytest --cov=.
```

---
If you have questions or need deeper internals (e.g., changing embeddings, swapping vector store, modifying prompt templates), start from `rag_pipeline.py` and `vector_store.py`—they are the heart of the system.
